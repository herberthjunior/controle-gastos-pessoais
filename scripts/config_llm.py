#!/usr/bin/env python3
"""
M√≥dulo de Configura√ß√£o do LLM - Sistema de Controle de Gastos Pessoais
Configura e testa a conex√£o com Gemini 2.5 Flash via OpenAI client.
"""

import os
from openai import OpenAI
from typing import Optional, Dict, Any

class ConfigLLM:
    def __init__(self):
        self.client = None
        self.modelo = "gpt-4.1-mini"  # Modelo dispon√≠vel via proxy Manus
        self.configurado = False
    
    def configurar_cliente(self) -> bool:
        """
        Configura o cliente OpenAI para acessar o Gemini via proxy.
        
        Returns:
            True se configurado com sucesso, False caso contr√°rio.
        """
        try:
            # As vari√°veis de ambiente j√° est√£o configuradas
            api_key = os.getenv('OPENAI_API_KEY')
            base_url = os.getenv('OPENAI_BASE_URL')
            
            if not api_key or not base_url:
                print("‚ùå Configura√ß√£o da API n√£o encontrada nas vari√°veis de ambiente")
                return False
            
            # Criar cliente OpenAI configurado para o proxy Manus
            self.client = OpenAI()  # Usa automaticamente as vari√°veis de ambiente
            
            print("‚úÖ Cliente LLM configurado com sucesso")
            print(f"   Modelo: {self.modelo}")
            print(f"   Endpoint: {base_url}")
            
            self.configurado = True
            return True
            
        except Exception as e:
            print(f"‚ùå Erro ao configurar cliente: {e}")
            return False
    
    def testar_conexao(self) -> bool:
        """
        Testa a conex√£o com a API fazendo uma requisi√ß√£o simples.
        
        Returns:
            True se o teste passou, False caso contr√°rio.
        """
        if not self.configurado:
            print("‚ùå LLM n√£o configurado. Execute configurar_cliente() primeiro.")
            return False
        
        try:
            print("üîÑ Testando conex√£o com a API...")
            
            # Fazer uma requisi√ß√£o simples de teste
            response = self.client.chat.completions.create(
                model=self.modelo,
                messages=[
                    {"role": "user", "content": "Responda apenas: 'Teste OK'"}
                ],
                max_tokens=10,
                temperature=0
            )
            
            if response and response.choices and response.choices[0].message.content:
                resposta = response.choices[0].message.content.strip()
                print(f"‚úÖ Teste de conex√£o bem-sucedido")
                print(f"   Resposta: {resposta}")
                return True
            else:
                print("‚ùå Resposta vazia da API")
                return False
                
        except Exception as e:
            print(f"‚ùå Erro no teste de conex√£o: {e}")
            return False
    
    def classificar_gasto(self, descricao: str) -> Optional[str]:
        """
        Classifica um gasto usando o LLM.
        
        Args:
            descricao: Descri√ß√£o do gasto para classificar
            
        Returns:
            Categoria classificada ou None em caso de erro
        """
        if not self.configurado:
            print("‚ùå LLM n√£o configurado")
            return None
        
        try:
            # Prompt para classifica√ß√£o de gastos
            prompt = f"""Voc√™ √© um assistente especializado em classifica√ß√£o de gastos pessoais.

Classifique o gasto a seguir em UMA das categorias abaixo:

CATEGORIAS DISPON√çVEIS:
- Alimenta√ß√£o
- Transporte
- Moradia
- Sa√∫de
- Educa√ß√£o
- Lazer
- Compras
- Servi√ßos
- Investimentos
- Outros

DESCRI√á√ÉO DO GASTO: {descricao}

INSTRU√á√ïES:
- Responda APENAS com o nome da categoria
- Use exatamente um dos nomes listados acima
- N√£o adicione explica√ß√µes ou coment√°rios

CATEGORIA:"""

            response = self.client.chat.completions.create(
                model=self.modelo,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=20,
                temperature=0
            )
            
            if response and response.choices and response.choices[0].message.content:
                categoria = response.choices[0].message.content.strip()
                return categoria
            else:
                return None
                
        except Exception as e:
            print(f"‚ùå Erro na classifica√ß√£o: {e}")
            return None
    
    def obter_configuracao(self) -> Dict[str, Any]:
        """
        Retorna informa√ß√µes sobre a configura√ß√£o atual.
        
        Returns:
            Dicion√°rio com informa√ß√µes da configura√ß√£o
        """
        return {
            'configurado': self.configurado,
            'cliente_inicializado': bool(self.client),
            'modelo': self.modelo,
            'api_key_definida': bool(os.getenv('OPENAI_API_KEY')),
            'base_url': os.getenv('OPENAI_BASE_URL')
        }

def main():
    """Fun√ß√£o principal para teste e configura√ß√£o inicial."""
    print("=== CONFIGURA√á√ÉO DO LLM (GEMINI 2.5 FLASH) ===")
    
    config = ConfigLLM()
    
    # Configurar cliente
    if not config.configurar_cliente():
        return False
    
    # Testar conex√£o
    if not config.testar_conexao():
        return False
    
    # Teste de classifica√ß√£o
    print("\nüß™ Testando classifica√ß√£o de gastos...")
    
    exemplos_teste = [
        "APPLE COM BILL SAO PAULO BRA",
        "99Food IFOOD CLUB Osasco BRA", 
        "POSTO SHELL COMBUSTIVEL",
        "FARMACIA DROGA RAIA"
    ]
    
    for exemplo in exemplos_teste:
        categoria = config.classificar_gasto(exemplo)
        print(f"   '{exemplo}' ‚Üí {categoria}")
    
    # Exibir configura√ß√£o final
    print("\nüìä Configura√ß√£o final:")
    info = config.obter_configuracao()
    for chave, valor in info.items():
        print(f"   {chave}: {valor}")
    
    print("\n‚úÖ LLM configurado e pronto para classifica√ß√£o de gastos!")
    return True

if __name__ == "__main__":
    main()
